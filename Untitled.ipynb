{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c243a-8183-47c8-92cf-97150cbafc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST data... (this might take a minute)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load Data\n",
    "print(\"Loading MNIST data... (this might take a minute)\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Subset for speed? (Optional: Uncomment next line to use only 10k samples for testing code)\n",
    "# X, y = X[:10000], y[:10000]\n",
    "\n",
    "# 2. Split Data (80% Train, 20% Test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Scale Data (Crucial for PCA and ANN/SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Create PCA Dataset\n",
    "# Retain 95% of variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original feature count: {X_train_scaled.shape[1]}\")\n",
    "print(f\"PCA feature count (95% variance): {X_train_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69cc16-e2a7-4d5f-ba3a-ad265878c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=50),\n",
    "    \"SVM\": SVC(kernel='rbf'), # RBF is standard for MNIST\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200, solver='lbfgs'),\n",
    "    \"Classification Tree\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "def evaluate_models(dataset_name, X_tr, X_te, y_tr, y_te):\n",
    "    print(f\"\\n--- Results for {dataset_name} Dataset ---\")\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Predict\n",
    "        pred = model.predict(X_te)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = accuracy_score(y_te, pred)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        print(f\"{name}: Accuracy = {acc:.4f} (Time: {duration:.2f}s)\")\n",
    "\n",
    "# Run on Full Dataset (Scaled)\n",
    "evaluate_models(\"FULL (Scaled)\", X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# Run on PCA Dataset\n",
    "evaluate_models(\"PCA (Compressed)\", X_train_pca, X_test_pca, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d371b-e37c-48c2-9879-3166caab4114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import homogeneity_score, completeness_score\n",
    "\n",
    "def run_clustering(dataset_name, X_data, y_true):\n",
    "    print(f\"\\n--- K-Means on {dataset_name} ---\")\n",
    "    \n",
    "    # We look for 10 clusters (digits 0-9)\n",
    "    kmeans = KMeans(n_clusters=10, n_init=10, random_state=42)\n",
    "    kmeans.fit(X_data)\n",
    "    \n",
    "    # Metrics\n",
    "    # Homogeneity: Each cluster contains only members of a single class\n",
    "    h_score = homogeneity_score(y_true, kmeans.labels_)\n",
    "    \n",
    "    print(f\"Homogeneity Score: {h_score:.4f}\")\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "# Run on Full Data\n",
    "kmeans_full = run_clustering(\"FULL Data\", X_train_scaled, y_train)\n",
    "\n",
    "# Run on PCA Data\n",
    "kmeans_pca = run_clustering(\"PCA Data\", X_train_pca, y_train)\n",
    "\n",
    "# Optional: Visualize the centers (What does the machine think the 'average' digit looks like?)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "centers = scaler.inverse_transform(kmeans_full.cluster_centers_) # Un-scale to see images\n",
    "for ax, center in zip(axes.ravel(), centers):\n",
    "    ax.imshow(center.reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"K-Means Centroids (Full Data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fef8e-c9bd-4073-b364-70d84dc0eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "def visualize_model_internals(models_dict, X_train, y_train, X_test):\n",
    "    \"\"\"\n",
    "    Visualizes the internal workings of specific models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Visualize Logistic Regression Weights ---\n",
    "    # This shows the \"ghostly\" templates the model matched against the digits\n",
    "    if \"Logistic Regression\" in models_dict:\n",
    "        model = models_dict[\"Logistic Regression\"]\n",
    "        print(\"\\n--- Logistic Regression Weights (The 'Templates') ---\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "        coefs = model.coef_.reshape(10, 28, 28) # Reshape 784 weights back to 28x28\n",
    "        \n",
    "        for i, ax in enumerate(axes.ravel()):\n",
    "            ax.imshow(coefs[i], cmap='seismic', vmin=-0.5, vmax=0.5)\n",
    "            ax.set_title(f\"Digit {i}\")\n",
    "            ax.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # --- 2. Visualize Decision Tree Logic ---\n",
    "    # This draws the top few decisions the tree makes\n",
    "    if \"Classification Tree\" in models_dict:\n",
    "        model = models_dict[\"Classification Tree\"]\n",
    "        print(\"\\n--- Decision Tree (Top 3 Levels) ---\")\n",
    "        \n",
    "        plt.figure(figsize=(20, 8))\n",
    "        tree.plot_tree(model, max_depth=3, feature_names=[f\"px_{i}\" for i in range(784)], \n",
    "                       class_names=[str(i) for i in range(10)], filled=True, fontsize=10)\n",
    "        plt.show()\n",
    "\n",
    "    # --- 3. Visualize KNN (Query vs Neighbors) ---\n",
    "    # Shows a test digit and the 5 closest training digits that voted on it\n",
    "    if \"K-Nearest Neighbors\" in models_dict:\n",
    "        model = models_dict[\"K-Nearest Neighbors\"]\n",
    "        print(\"\\n--- KNN: Test Image vs. 5 Nearest Neighbors ---\")\n",
    "        \n",
    "        # Pick a random test image\n",
    "        test_idx = 0 \n",
    "        distances, indices = model.kneighbors(X_test[test_idx].reshape(1, -1))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 6, figsize=(12, 3))\n",
    "        \n",
    "        # Plot Test Image\n",
    "        axes[0].imshow(X_test[test_idx].reshape(28, 28), cmap='gray')\n",
    "        axes[0].set_title(\"Test Image\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Plot Neighbors\n",
    "        for i, neighbor_idx in enumerate(indices[0]):\n",
    "            axes[i+1].imshow(X_train[neighbor_idx].reshape(28, 28), cmap='gray')\n",
    "            axes[i+1].set_title(f\"Neighbor {i+1}\")\n",
    "            axes[i+1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# NOTE: You must run this using the \"Full\" (not PCA) models for the images to make sense!\n",
    "# Pass the dictionary of trained models from Part 2 here:\n",
    "visualize_model_internals(models, X_train_scaled, y_train, X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243ad82-2ff3-490d-8d59-3acb8e923575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Container to store results\n",
    "results = []\n",
    "\n",
    "def train_and_log(dataset_name, X_tr, X_te, y_tr, y_te):\n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_tr, y_tr)\n",
    "        end = time.time()\n",
    "        \n",
    "        acc = model.score(X_te, y_te)\n",
    "        time_taken = end - start\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Time (s)\": time_taken\n",
    "        })\n",
    "        print(f\"{dataset_name} - {name}: {acc:.4f} in {time_taken:.2f}s\")\n",
    "\n",
    "# Run training (as before)\n",
    "train_and_log(\"Full Data\", X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "train_and_log(\"PCA Data\", X_train_pca, X_test_pca, y_train, y_test)\n",
    "\n",
    "# 2. Visualize Performance\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Scatter plot: Time vs Accuracy\n",
    "sns.scatterplot(data=df_results, x=\"Time (s)\", y=\"Accuracy\", hue=\"Dataset\", style=\"Model\", s=200)\n",
    "\n",
    "plt.title(\"Model Performance: Accuracy vs Training Time\", fontsize=16)\n",
    "plt.xscale('log') # Log scale because SVM/KNN might take 100x longer than Logistic Regression\n",
    "plt.xlabel(\"Training Time (seconds) - Log Scale\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy Score\", fontsize=12)\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
